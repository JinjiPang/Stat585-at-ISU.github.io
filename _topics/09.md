---
layout: blog
topic: "09"
short-topic: Web scraping etiquette ...
due-date: 2022-04-14
root: ../../
---

## Prompt:

With great power comes great responsibility - a large part of the web is based on data and services that scrape those data. 
Now that we know how to apply scraping mechanisms, we need to think about how to apply those skills without becoming a burden to the internet society.

Find sources on ethical web scraping - some readings that might help you get started with that are: 

  - [James Densmore](https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01)

  - R package [polite](https://github.com/dmi3kno/polite)

  - [JAMI @ EMPIRICAL](https://www.empiricaldata.org/dataladyblog/a-guide-to-ethical-web-scraping)


After reading through some of the ethics essays
write a blog post addressing the following questions: 

1. **What are your main three takeaways for ethical web scraping? - Give examples, or cite your sources.**
2. **What is a ROBOTS.TXT file? Identify one instance and explain what it allows/prevents.**
3. **Use one of the examples from class (or one of your own examples) for scraping a website and implement the same scrape using the `polite` package. What differences do you find?**

Instructions:

You will receive an invite to a github classroom assignment by email. 
Accept the assignment and follow the instructions.


{% assign num_posts = site.blog | size %}
{% if num_posts > 0 %}
## Class posts:

<ul>
{% for post in site.blog %}
  {% if page.topic == post.topic %}
  <li><a href="{{ post.url }}">{{ post.title }} by {{ post.author }}</a></li>
  {% endif %}
{% endfor %}
</ul>
{% endif %}
